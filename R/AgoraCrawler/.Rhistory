data.nomiss <- data.nomiss[,-c(1, 2, 13, 15)]
# 테스트, 트레이닝 데이터 분리
set.seed(1234) # 난수고정
ind <- sample(2, nrow(data.nomiss), replace=TRUE, prob=c(0.8, 0.2))
training <- data.nomiss[ind==1,]
testing <- data.nomiss[ind==2,]
control = trainControl(method="repeatedcv", number=10, repeats=5)
set.seed(1234)
model.rf = train(target2~., method="rf", trControl=control, data=training)
set.seed(1234)
model.svm = train(target2~., method="svmRadial", trControl=control, data=training)
set.seed(1234)
model.gbm = train(target2~., method="gbm", trControl=control, data=training)
bwplot(results)
results <- resamples(list(RF=model.rf, SVM=model.svm, GBM=model.gbm))
summary(results)
bwplot(results)
dotplot(results)
data.nomiss$up <- ifelse(data.nomiss$target2 > 0, 1, 0)
data.nomiss$up <- as.factor(data.nomiss$up)
data.nomiss = data.nomiss[ , -which(names(data.nomiss) %in% c("target2"))]
summary(data.nomiss)
set.seed(1234) # 난수고정
ind <- sample(2, nrow(data.nomiss), replace=TRUE, prob=c(0.8, 0.2))
training <- data.nomiss[ind==1,]
testing <- data.nomiss[ind==2,]
control = trainControl(method="repeatedcv", number=10, repeats=5)
set.seed(1234)
model.rf = train(up~., method="rf", trControl=control, data=training)
set.seed(1234)
model.svm = train(up~., method="svmRadial", trControl=control, data=training)
set.seed(1234)
model.gbm = train(up~., method="gbm", trControl=control, data=training)
results <- resamples(list(RF=model.rf, SVM=model.svm, GBM=model.gbm))
summary(results)
bwplot(results)
dotplot(results)
results <- resamples(model.rf)
results <- resamples(list(RF=model.rf))
model.rf
?train
rf = randomForest(x=training[,!(names(training) %in% "up")], y=training$up, ntree=100, importance=T, do.trace=5)
rf = randomForest(x=training[,!(names(training) %in% "up")], y=training$up, ntree=100, importance=T, do.trace=5)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
head(testing$up,1000)
importance(rf)
library(ggplot2)
library(dplyr)
library(caret)
library(party)
library(rpart)
library(partykit)
library(e1071)
library(kernlab)
raw.data <- read.csv("D:/futures/yw_1007/csv_20121023_201509215_her.csv",
sep=",",
header = F)
data <- raw.data
# 빈 컬럼 제거
data <- data[,-71]
## 변수명 수정
# V13 등락
# V14 target  <- 타겟변수: 내일종가 - 오늘종가(미래 값)
# V15 등락률
colnames(data)[13] <- "target1"
colnames(data)[14] <- "target2"
colnames(data)[15] <- "target"
# 상승 비율
#data$up_ratio <- (data$target2)/V6
#계산법
#(내일종가-오늘종가)/오늘종가
# 10% 상승 여부
#data$up10 <- ifelse(data$up_ratio >= 0.1, 1, 0)
#data$up10 <- as.factor(data$up10)
#is.factor(data$up10)
#View(data)
# 결측 데이터 제거
data.nomiss <- na.omit(data)
## 불필요 변수 및 타겟 직접관련변수 제외
# date, 종목, target, target1, target2
data.nomiss$up <- ifelse(data.nomiss$target >= 0.1, A, ifelse(data.nomiss$target >= 0, B,ifelse(data.nomiss$target >= -0.1, C,D)))
data.nomiss$up <- as.factor(data.nomiss$up)
data.nomiss <- data.nomiss[,-c(1, 2, 13, 14, 15)]
# 테스트, 트레이닝 데이터 분리
set.seed(1234) # 난수고정
ind <- sample(2, nrow(data.nomiss), replace=TRUE, prob=c(0.8, 0.2))
training <- data.nomiss[ind==1,]
testing <- data.nomiss[ind==2,]
# best model rf
rf = randomForest(x=training[,!(names(training) %in% "up")], y=training$up, ntree=100, importance=T, do.trace=5)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
summanry(training)
summary(training)
summary(data.nomiss$up)
library(ggplot2)
library(dplyr)
library(caret)
library(party)
library(rpart)
library(partykit)
library(e1071)
library(kernlab)
raw.data <- read.csv("D:/futures/yw_1007/csv_20121023_201509215_her.csv",
sep=",",
header = F)
data <- raw.data
# 빈 컬럼 제거
data <- data[,-71]
## 변수명 수정
# V13 등락
# V14 target  <- 타겟변수: 내일종가 - 오늘종가(미래 값)
# V15 등락률
colnames(data)[13] <- "target1"
colnames(data)[14] <- "target2"
colnames(data)[15] <- "target"
# 상승 비율
#data$up_ratio <- (data$target2)/V6
#계산법
#(내일종가-오늘종가)/오늘종가
# 10% 상승 여부
#data$up10 <- ifelse(data$up_ratio >= 0.1, 1, 0)
#data$up10 <- as.factor(data$up10)
#is.factor(data$up10)
#View(data)
# 결측 데이터 제거
data.nomiss <- na.omit(data)
summary(data.nomiss)
data.nomiss$up <- ifelse(data.nomiss$target >= 0.1, A, ifelse(data.nomiss$target >= 0, B,ifelse(data.nomiss$target >= -0.1, C,D)))
data.nomiss$up <- ifelse(data.nomiss$target >= 0.1, 'A', ifelse(data.nomiss$target >= 0, 'B',ifelse(data.nomiss$target >= -0.1, 'C','D')))
data.nomiss$up <- as.factor(data.nomiss$up)
data.nomiss <- data.nomiss[,-c(1, 2, 13, 14, 15)]
# 테스트, 트레이닝 데이터 분리
set.seed(1234) # 난수고정
ind <- sample(2, nrow(data.nomiss), replace=TRUE, prob=c(0.8, 0.2))
training <- data.nomiss[ind==1,]
testing <- data.nomiss[ind==2,]
summary(training$up)
rf = randomForest(x=training[,!(names(training) %in% "up")], y=training$up, ntree=100, importance=T, do.trace=5)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
rf = randomForest(x=training[,!(names(training) %in% "up")], y=training$up, ntree=100, importance=T, do.trace=10)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
rf = randomForest(x=training[,!(names(training) %in% "up")], y=training$up, ntree=100, do.trace=5)
?raondomForest
?randomForest
rf = randomForest(up~.,data=training, ntree=100, do.trace=5)
rf = randomForest(up~.,data=training, do.trace=5)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
summary(training$up)
install.packages("corrplot")
require(corrplot)
corrplot(cor(data.nomiss), order="hclust", tl.cex=.6, addrect=3, rect.col="red", is.corr=F)
cor
lc = findLinearCombos(data.nomiss)
summary(data.nomiss)
cor1 = findCorrelation(cor(data.nomiss), cutoff=.99, verbose=T, exact=T)
data.nomiss <- na.omit(data)
cor1 = findCorrelation(cor(data.nomiss), cutoff=.99, verbose=T, exact=T)
summary(data.nomiss)
data.nomiss <- data.nomiss[,-c(1, 2, 13, 14, 15)]
cor1 = findCorrelation(cor(data.nomiss), cutoff=.99, verbose=T, exact=T)
data.nomiss <- na.omit(data)
## 불필요 변수 및 타겟 직접관련변수 제외
# date, 종목, target, target1, target2
data.nomiss$up <- ifelse(data.nomiss$target >= 0.1, 'A', ifelse(data.nomiss$target >= 0, 'B',ifelse(data.nomiss$target >= -0.1, 'C','D')))
data.nomiss$up <- as.factor(data.nomiss$up)
data.nomiss <- data.nomiss[,-c(1, 2, 13, 14, 15)]
data.vari = data.nomiss[, -which(colnames(data.nomiss) %in% c("up"))]
data.vari
summary(data.vari)
cor1 = findCorrelation(cor(data.vari), cutoff=.99, verbose=T, exact=T)
data.nomiss <- data.nomiss[,-c(4, 59, 45, 10, 6,23,2)]
set.seed(1234) # 난수고정
ind <- sample(2, nrow(data.nomiss), replace=TRUE, prob=c(0.8, 0.2))
training <- data.nomiss[ind==1,]
testing <- data.nomiss[ind==2,]
# best model rf
rf = randomForest(up~.,data=training, do.trace=5)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
rf = train(up~., method="rf", data=training)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
rf = randomForest(up~.,data=training, importance=TRUE, do.trace=5)
pred <- predict(rf, testing)
importance(rf)
confusionMatrix(pred, testing$up)
rf = randomForest(up~.,data=training, importance=TRUE, do.trace=5, ntree=10)
control = trainControl(method="repeatedcv", number=10, repeats=5)
rf = train(up~., method="rf", data=training, trControl=control)
pred <- predict(rf, testing)
confusionMatrix(pred, testing$up)
pred
library(ggplot2)
library(dplyr)
library(caret)
require(corrplot)
raw.data <- read.csv("D:/futures/yw_1007/csv_20121023_201509215_her.csv",
sep=",",
header = F)
data <- raw.data
# 빈 컬럼 제거
data <- data[,-71]
## 변수명 수정
# V13 등락
# V14 target  <- 타겟변수: 내일종가 - 오늘종가(미래 값)
# V15 등락률
colnames(data)[13] <- "target1"
colnames(data)[14] <- "target2"
colnames(data)[15] <- "target"
# 상승 비율
#data$up_ratio <- (data$target2)/V6
#계산법
#(내일종가-오늘종가)/오늘종가
# 10% 상승 여부
#data$up10 <- ifelse(data$up_ratio >= 0.1, 1, 0)
#data$up10 <- as.factor(data$up10)
#is.factor(data$up10)
#View(data)
# 결측 데이터 제거
data.nomiss <- na.omit(data)
## 불필요 변수 및 타겟 직접관련변수 제외
# date, 종목, target, target1, target2
data.nomiss$up <- ifelse(data.nomiss$target >= 1, 'A', ifelse(data.nomiss$target >= 0, 'B',ifelse(data.nomiss$target >= -1, 'C','D')))
data.nomiss$up <- as.factor(data.nomiss$up)
data.nomiss <- data.nomiss[,-c(1, 2, 13, 14, 15)]
data.vari = data.nomiss[, -which(colnames(data.nomiss) %in% c("up"))]
cor1 = findCorrelation(cor(data.vari), cutoff=.99, verbose=T, exact=T)
# 테스트, 트레이닝 데이터 분리
set.seed(1234) # 난수고정
ind <- sample(2, nrow(data.nomiss), replace=TRUE, prob=c(0.8, 0.2))
training <- data.nomiss[ind==1,]
testing <- data.nomiss[ind==2,]
# best model rf
#rf = randomForest(up~.,data=training, importance=TRUE, do.trace=5)
control = trainControl(method="repeatedcv", number=10, repeats=5)
rf = train(up~., method="rf", data=training, trControl=control)
pred = predict(rf, testing)
confusionMatrix(pred, testing$up)
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(ggplot2)
consumer_key = "psGG42l9DFZSTVijML6u2pFHb"
consumer_secret = "tn9Ma0RSSSEwA4gu1XzfMgqgtSiUunPAgoXEYx1MkZxQxNcSDa"
access_token = "17185110-XsunnUvJISlx6LqicCwxgFrtARlBEkEy6oFXZtix6"
access_token_secret = "Pi5uaCENNaDp3wTVnCMIgHcRzUr8bZhqsUaXC2soLYOS6"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
keyword = enc2utf8("국정화")
tweets = searchTwitter(keyword, lang='ko', n=1000)
library(KoNLP)
library(tm)
library(rJava)
library(rJava)
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(ggplot2)
consumer_key = "psGG42l9DFZSTVijML6u2pFHb"
consumer_secret = "tn9Ma0RSSSEwA4gu1XzfMgqgtSiUunPAgoXEYx1MkZxQxNcSDa"
access_token = "17185110-XsunnUvJISlx6LqicCwxgFrtARlBEkEy6oFXZtix6"
access_token_secret = "Pi5uaCENNaDp3wTVnCMIgHcRzUr8bZhqsUaXC2soLYOS6"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
keyword = enc2utf8("국정화")
1tweets = searchTwitter(keyword, lang='ko', n=1000)
tweets = searchTwitter(keyword, lang='ko', n=1000)
library(KoNLP)
library(tm)
nDocs <- length(tweets)
df <- twListToDF(tweets)
removeTwit <- function(x) {gsub("@[[:graph:]]*", "", x)}
removeTwit
df$text
df$ptext <- sapply(df$text, removeTwit)
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
df$ptext <- sapply(df$ptext, removeURL)
useSejongDic()
?useSejongDic
df$ptext <- sapply(df$ptext, function(x) {paste(extractNoun(x), collapse=" ")})
df$ptext
?tm_map
myCorpus_ <- Corpus(VectorSource(df$ptext))
myCorpus_
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, tolower)
myStopwords <- c(stopwords('english'), "rt")
myCorpus_ <-tm_map(myCorpus_, removeWords, myStopwords)
myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(2,Inf)))
myTdm <- TermDocumentMatrix(myCorpus_, control=list(wordLengths=c(2,Inf)))
?inherits
myCorpus <- tm_map(myCorpus_, PlainTextDocument)
myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(2,Inf)))
myTdm
findFreqTerms(myTdm, lowfreq=10)
findAssocs(myTdm,'lg',0.25)
?findAssocs
findFreqTerms(myTdm, highfreq=10)
?findFreqTerms
findFreqTerms(myTdm, lowfreq=100)
findFreqTerms(myTdm, lowfreq=50)
?findAssocs
findAssocs(myTdm,c('박근혜','교과서','국사'),c(0.7,0.1,0.3))
findAssocs(myTdm,c('친일','독재','국사'),c(0.5,0.4,0.3))
library(ggplot2)
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency,termFrequency>=10)
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar() + coord_flip()
termFrequency <- subset(termFrequency,termFrequency>=40)
termFrequency <- subset(termFrequency,termFrequency>=40)
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar() + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq), stat='bin') + geom_bar() + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq), stat='identity') + geom_bar() + coord_flip()
?"ggplot"
?aes
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(x=term, y=freq)) + geom_bar() + coord_flip()
summary(termFrequency)
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(x=term)) + geom_bar(aes(y=freq)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq) + geom_bar(aes(y= ..density..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar(aes(y= ..density..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..ndensity..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..density..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..scaled..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..stat_bin..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..density..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar(aes(y= ..density..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar(aes(y= ..scaled..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..scaled..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..stat_bin..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_histogram(aes(y= ..identity..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar(aes(y= ..identity..)) + coord_flip()
ggplot(data.frame(term = names(termFrequency), freq=termFrequency), aes(term, freq)) + geom_bar(aes(y= ..count..)) + coord_flip()
termFrequency
library(wordcloud)
m <- as.matrix(myTdm)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
set.seed(375)
pal <- brewer.pal(8,"Dark2")
?brewer.pal
wordcloud(words=names(wordFreq),freq=wordFreq,min.freq=10,random.order=F, rot.per=.1,colors=pal)
removeURL <- function(x) { gsub("https://[[:graph:]]*", "", x)}
df$ptext <- sapply(df$ptext, removeURL)
library(wordcloud)
m <- as.matrix(myTdm)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
set.seed(375)
pal <- brewer.pal(8,"Dark2")
wordcloud(words=names(wordFreq),freq=wordFreq,min.freq=10,random.order=F, rot.per=.1,colors=pal)
nDocs <- length(tweets)
## data frame 으로 변환
df <- twListToDF(tweets)
## 불필요한 단어 제거
removeTwit <- function(x) {gsub("@[[:graph:]]*", "", x)}
df$ptext <- sapply(df$text, removeTwit)
removeURL <- function(x) { gsub("http://[[:graph:]]*", "", x)}
df$ptext <- sapply(df$ptext, removeURL)
removeURL <- function(x) { gsub("https://[[:graph:]]*", "", x)}
df$ptext <- sapply(df$ptext, removeURL)
# 사전지정
useSejongDic()
# 명사 추출
df$ptext <- sapply(df$ptext, function(x) {paste(extractNoun(x), collapse=" ")})
df$ptext <- sapply(df$ptext, function(x) {paste(extractNoun(x), collapse=" ")})
# corpus 생성
myCorpus_ <- Corpus(VectorSource(df$ptext))
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, tolower)
myStopwords <- c(stopwords('english'), "rt")
myCorpus_ <-tm_map(myCorpus_, removeWords, myStopwords)
myCorpus <- tm_map(myCorpus_, PlainTextDocument)
# matrix 생성
myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(2,Inf)))
findFreqTerms(myTdm, lowfreq=50)
findAssocs(myTdm,c('친일','독재','국사'),c(0.5,0.4,0.3))
library(wordcloud)
m <- as.matrix(myTdm)
wordFreq <- sort(rowSums(m),decreasing=TRUE)
set.seed(375)
pal <- brewer.pal(8,"Dark2")
wordcloud(words=names(wordFreq),freq=wordFreq,min.freq=10,random.order=F, rot.per=.1,colors=pal)
myTdm2<-removeSparseTerms(myTdm,sparse=0.95)
m2<-as.matrix(myTdm2)
distMatrix<-dist(scale(m2))
fit<-hclust(distMatrix,method="ward")
plot(fit)
rect.hclust(fit,k=10)
m3 <- t(m2)
k <- 4
kmres <- kmeans(m3, k)
round(kmres$centers, digits=3)
for(i in 1:k){
cat(paste("cluster ", i, " : ", sep=""))
s <- sort(kmres$centers[i, ], decreasing=T)
cat(names(s)[1:3], "\n")
#print(head(rdmTweets[which(kmres$cluster ==i)],n=3))
}
## @knitr kmedoid
library(fpc)
pamResult <- pamk(m3, metric="manhattan")
library(cluster)
install.packages("HSAUR")
library(HSAUR)
dissE <- daisy(m3)
dE2 <- dissE^2
sk2 <- silhouette(kmres$cl, dE2)
plot(sk2)
plot(kmres)
plot(kmres,m3)
m3
m2
View(m2)
myTdm
?TermDocumentMatrix
myTdm
View(myTdm)
findFreqTerms(myTdm, lowfreq=50)
m
View(m)
View(m)
?g
?t
head(m2)
head(m3)
require(vegan)
m3_dist <- dist(m3)
# Multidimensional scaling
cmd <- cmdscale(m3_dist)
groups <- levels(factor(kmres$cluster))
ordiplot(cmd, type = "n")
??ordiplot
install.packages("vgen")
install.packages("vegan")
ordiplot(cmd, type = "n")
library(vegan)
ordiplot(cmd, type = "n")
cols <- c("steelblue", "darkred", "darkgreen", "pink")
for(i in seq_along(groups)){
points(cmd[factor(kmres$cluster) == groups[i], ], col = cols[i], pch = 16)
}
ordispider(cmd, factor(kclus$cluster), label = TRUE)
ordispider(cmd, factor(kmres$cluster), label = TRUE)
ordihull(cmd, factor(kmres$cluster), lty = "dotted")
install.packages("fpc")
library(fpc)
pamResult <- pamk(m3, metric="manhattan")
(k <- pamResult$nc)
pamResult <- pamResult$pamobject
#print cluster medoids
for(i in 1:k){
cat(paste("cluster",i,":"))
cat(colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)],"\n")
# print tweets in cluster i
#print(rdmTweets[pamResult$clustering==i])
}
?pamk
pamResult
pamResult@cluster
getwd*()
getwd()
?gsub
setwd("D:/kodb/R/AgoraCrawler")
library(RCurl)
library(XML)
baseUrl = "http://agora.daum.net/nsearch/total?query="
sortType = "&sort_type=1"
searchword = "두산베어스"
savefile = "result.csv"
baseUrl
url = gsub(" ","", paste(baseUrl,searchword,sortType))
page <- getURL(url)
page
doc = htmlTreeParse(page, useInternalNodes = T)
doc
contents <- xpathSApply(doc, "//*[@class='sResult']//dl")
body_content <- NULL
body_table <- NULL
for (i in 1:length(contents)){
url <- xpathSApply(contents[[i]], "dt/a/@href")
date <- xpathSApply(contents[[i]],"dt/span/text()")
title <- xpathSApply(contents[[i]],"dd/text()")
body_content <- cbind(url[[1]], xmlValue(date[[1]]), xmlValue(title[[1]]))
body_table <- rbind(body_table, body_content)
}
write.csv(body_table, savefile, fileEncoding = "UTF-8")
library(twitteR)
library(ROAuth)
library(plyr)
library(stringr)
library(ggplot2)
consumer_key = "psGG42l9DFZSTVijML6u2pFHb"
consumer_secret = "tn9Ma0RSSSEwA4gu1XzfMgqgtSiUunPAgoXEYx1MkZxQxNcSDa"
access_token = "17185110-XsunnUvJISlx6LqicCwxgFrtARlBEkEy6oFXZtix6"
access_token_secret = "Pi5uaCENNaDp3wTVnCMIgHcRzUr8bZhqsUaXC2soLYOS6"
setup_twitter_oauth(consumer_key,consumer_secret,access_token,access_token_secret)
keyword = enc2utf8("국정화")
tweets = searchTwitter(keyword, lang='ko', n=1000)
summary(tweets)
head(tweets)
